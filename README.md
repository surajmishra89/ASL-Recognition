# ASL-Recognition
A real-time American Sign Language (ASL) recognition system that converts hand gestures into text and speech using computer vision and deep learning. Built with Python, OpenCV, TensorFlow, and a custom-trained CNN model, this application empowers accessibility by enabling hands-free communication through sign language.

ğŸš€ Features
âœ… Core Functionalities
Hand Gesture Recognition: Real-time detection of hand gestures using 21-point landmarks extracted via cvzone's HandDetector.
Deep Learning-based Alphabet Prediction: A Convolutional Neural Network (CNN) trained on 26 classes (A-Z) using skeletonized hand images, achieving >95% validation accuracy.
Text-to-Speech Integration: Converts the constructed sentence from hand signs to audible speech using pyttsx3.
Word and Sentence Suggestions:
Next Word Prediction using an NLTK-trained Trigram Language Model on the Brown Corpus.
Auto Spell Correction with SymSpell based on dictionary frequency and edit distance.
Gesture Controls for Text Manipulation:
âœ‹ Open Hand â†’ Insert Space
ğŸ‘ Thumb Out â†’ Backspace
Hands-Free Operation: Completely keyboard-free communication using gestures.
ğŸ§  Model Architecture
Input: 55x55 grayscale images of skeletonized hand gestures
Layers:
3 Conv2D layers + BatchNormalization + MaxPooling + Dropout
Fully Connected Layers: 512, 256 neurons + Dropout
Output: Softmax layer with 26 nodes (A-Z)
Training Techniques:
Data Augmentation: Rotation, Zoom, Shear, Flip
Early Stopping and ReduceLROnPlateau
ModelCheckpoint for best validation accuracy
ğŸ“¸ GUI Overview
Built using Tkinter with the following features:

Live camera feed panel
Skeleton drawing panel
Current prediction with confidence level
Sentence builder with:
Word suggestions
Next word suggestions
Control buttons: Speak, Auto-Correct, Backspace, Clear
ğŸ“ File Structure
â”œâ”€â”€ alphabetPred.py # Basic skeleton-to-letter prediction with OpenCV â”œâ”€â”€ final1pred.py # Full GUI-based ASL-to-speech system â”œâ”€â”€ handAcquisition.py # Data collection tool to save hand skeletons per letter â”œâ”€â”€ trainmodel.py # CNN training script with data augmentation â”œâ”€â”€ AtoZ_3.1/ # Dataset folder (26 folders for each alphabet) â”œâ”€â”€ model-bw.weights.h5 # Trained Keras model â”œâ”€â”€ best_model.h5 â”œâ”€â”€ model-bw.json # Model architecture in JSON format

ğŸ†• Unique Aspects
Skeleton-Based Image Generation: Uses structured 2D skeleton lines instead of raw RGB images, improving generalization and reducing noise.
Hands-Free Control: Users can insert space or backspace via gestures, making it usable by people with limited keyboard access.
Integrated Language Understanding: Context-aware suggestions using NLP techniques.
ğŸ§ª Future Enhancements
Support for dynamic gestures (motion-based signs)
Expansion to numbers and ASL-specific phrases
Web or mobile deployment (Flask/Kivy)
Multi-hand tracking and recognition
ğŸ›  Technologies Used
Languages & Libraries: Python, OpenCV, TensorFlow, Keras, NLTK, SymSpellPy, Tkinter, PIL, cvzone
Model Type: Custom CNN for image classification
NLP Tools: NLTK (Trigram language model), SymSpell (spell correction)
